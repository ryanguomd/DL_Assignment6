{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanguomd/DL_Assignment6/blob/main/Copy_of_09_Assigment_6_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ],
      "metadata": {
        "id": "CtuSrazlNYEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ],
      "metadata": {
        "id": "vriXNd_nL2q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Split into training (80%) and validation (20%)."
      ],
      "metadata": {
        "id": "Q5atve1sMH9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from collections import Counter\n",
        "from nltk.tokenize import sent_tokenize\n"
      ],
      "metadata": {
        "id": "QvKdt5EyMDug"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Frankenstein by Mary Shelley\n",
        "url = \"https://www.gutenberg.org/cache/epub/84/pg84.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Remove Project Gutenberg header and footer\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***\"\n",
        "\n",
        "if start_marker in text and end_marker in text:\n",
        "    content = text.split(start_marker)[1].split(end_marker)[0].strip()\n",
        "else:\n",
        "    raise ValueError(\"Start or end marker not found.\")"
      ],
      "metadata": {
        "id": "DBK2Ls6sUu5N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ],
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean text: lowercase and remove all punctuation except .,!,?\n",
        "punct_to_remove = string.punctuation.replace('.', '').replace('!', '').replace('?', '')\n",
        "table = str.maketrans('', '', punct_to_remove)\n",
        "cleaned_text = content.lower().translate(table)\n",
        "\n",
        "# Word-level tokenization\n",
        "tokens = cleaned_text.split()\n",
        "total_words = len(tokens)\n",
        "split_index = int(0.8 * total_words)\n",
        "train_tokens = tokens[:split_index]\n",
        "val_tokens = tokens[split_index:]\n",
        "\n",
        "# Build vocab\n",
        "word_counts = Counter(train_tokens)\n",
        "min_word_freq = 5\n",
        "vocab = [\"\"] + [word for word, count in word_counts.items() if count >= min_word_freq]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
        "unk_id = word_to_id[\"\"]\n",
        "\n",
        "# Encode\n",
        "train_ids = [word_to_id.get(word, unk_id) for word in train_tokens]\n",
        "val_ids = [word_to_id.get(word, unk_id) for word in val_tokens]"
      ],
      "metadata": {
        "id": "RvXRFVcbMLe9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ],
      "metadata": {
        "id": "jbTZs3OiMMNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the embedding layer\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=len(word_to_id),\n",
        "    output_dim=128,\n",
        "    input_length=10\n",
        ")"
      ],
      "metadata": {
        "id": "OXCK40l6MRld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c78317-62f7-46c6-b3a7-24c1e1a9e103"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ],
      "metadata": {
        "id": "qsXR4RZpMXMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerplexityCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        train_loss = logs.get(\"loss\")\n",
        "        val_loss = logs.get(\"val_loss\")\n",
        "\n",
        "        train_ppl = math.exp(train_loss) if train_loss else float(\"inf\")\n",
        "        val_ppl = math.exp(val_loss) if val_loss else float(\"inf\")\n",
        "\n",
        "        print(f\"\\n Epoch {epoch}:\")\n",
        "        print(f\"   - Training   Loss: {train_loss:.4f} | Perplexity: {train_ppl:.2f}\")\n",
        "        print(f\"   - Validation Loss: {val_loss:.4f} | Perplexity: {val_ppl:.2f}\")"
      ],
      "metadata": {
        "id": "linweGaUMg0T"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 10\n",
        "\n",
        "def create_sequences(ids, seq_len):\n",
        "    X, y = [], []\n",
        "    for i in range(seq_len, len(ids)):\n",
        "        X.append(ids[i - seq_len:i])\n",
        "        y.append(ids[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_train, y_train = create_sequences(train_ids, sequence_length)\n",
        "X_val, y_val = create_sequences(val_ids, sequence_length)"
      ],
      "metadata": {
        "id": "3DL3XnL9Vj5m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length = sequence_length, mask_zero=True),\n",
        "    LSTM(256, return_sequences = True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(256),\n",
        "    Dropout(0.2),\n",
        "    Dense(vocab_size, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer = AdamW(learning_rate = 5e-5, weight_decay = 1e-6),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor = 'val_loss', patience = 3, restore_best_weights = True),\n",
        "    ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 2),\n",
        "    PerplexityCallback()\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data = (X_val, y_val),\n",
        "    batch_size = 64,\n",
        "    epochs = 10,\n",
        "    callbacks = callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euRUdcuvVomx",
        "outputId": "94cbe647-6c29-4261-9d5d-c9d3e8e622c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.1617 - loss: 6.0346\n",
            " Epoch 0:\n",
            "   - Training   Loss: 5.4645 | Perplexity: 236.17\n",
            "   - Validation Loss: 4.9859 | Perplexity: 146.33\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 175ms/step - accuracy: 0.1617 - loss: 6.0340 - val_accuracy: 0.1910 - val_loss: 4.9859 - learning_rate: 5.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.1731 - loss: 5.1498\n",
            " Epoch 1:\n",
            "   - Training   Loss: 5.1304 | Perplexity: 169.08\n",
            "   - Validation Loss: 4.9744 | Perplexity: 144.67\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 189ms/step - accuracy: 0.1731 - loss: 5.1498 - val_accuracy: 0.1910 - val_loss: 4.9744 - learning_rate: 5.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.1740 - loss: 5.1206\n",
            " Epoch 2:\n",
            "   - Training   Loss: 5.1249 | Perplexity: 168.16\n",
            "   - Validation Loss: 4.9717 | Perplexity: 144.28\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 178ms/step - accuracy: 0.1740 - loss: 5.1206 - val_accuracy: 0.1910 - val_loss: 4.9717 - learning_rate: 5.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.1720 - loss: 5.1123\n",
            " Epoch 3:\n",
            "   - Training   Loss: 5.1204 | Perplexity: 167.40\n",
            "   - Validation Loss: 4.9709 | Perplexity: 144.16\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 174ms/step - accuracy: 0.1720 - loss: 5.1123 - val_accuracy: 0.1910 - val_loss: 4.9709 - learning_rate: 5.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.1718 - loss: 5.1190\n",
            " Epoch 4:\n",
            "   - Training   Loss: 5.1187 | Perplexity: 167.12\n",
            "   - Validation Loss: 4.9690 | Perplexity: 143.88\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 181ms/step - accuracy: 0.1718 - loss: 5.1190 - val_accuracy: 0.1910 - val_loss: 4.9690 - learning_rate: 5.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.1732 - loss: 5.1184\n",
            " Epoch 5:\n",
            "   - Training   Loss: 5.1163 | Perplexity: 166.73\n",
            "   - Validation Loss: 4.9699 | Perplexity: 144.01\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 184ms/step - accuracy: 0.1732 - loss: 5.1184 - val_accuracy: 0.1910 - val_loss: 4.9699 - learning_rate: 5.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.1727 - loss: 5.1078\n",
            " Epoch 6:\n",
            "   - Training   Loss: 5.1133 | Perplexity: 166.22\n",
            "   - Validation Loss: 4.9650 | Perplexity: 143.30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 181ms/step - accuracy: 0.1727 - loss: 5.1078 - val_accuracy: 0.1910 - val_loss: 4.9650 - learning_rate: 5.0000e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.1742 - loss: 5.0985\n",
            " Epoch 7:\n",
            "   - Training   Loss: 5.1049 | Perplexity: 164.83\n",
            "   - Validation Loss: 4.9554 | Perplexity: 141.94\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 184ms/step - accuracy: 0.1742 - loss: 5.0985 - val_accuracy: 0.1910 - val_loss: 4.9554 - learning_rate: 5.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.1736 - loss: 5.0965\n",
            " Epoch 8:\n",
            "   - Training   Loss: 5.0870 | Perplexity: 161.90\n",
            "   - Validation Loss: 4.9336 | Perplexity: 138.88\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 191ms/step - accuracy: 0.1736 - loss: 5.0965 - val_accuracy: 0.1910 - val_loss: 4.9336 - learning_rate: 5.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.1706 - loss: 5.0684\n",
            " Epoch 9:\n",
            "   - Training   Loss: 5.0546 | Perplexity: 156.75\n",
            "   - Validation Loss: 4.9009 | Perplexity: 134.41\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 173ms/step - accuracy: 0.1706 - loss: 5.0684 - val_accuracy: 0.1910 - val_loss: 4.9009 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a9b3c384d50>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible. If you have higher value (which is possible) try to draw conclusions, why doesn't it decrease to a lower value."
      ],
      "metadata": {
        "id": "Ggop4h4IMhMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ],
      "metadata": {
        "id": "cbvbBOp3MfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, num_tokens=50):\n",
        "    result = seed_text.lower().split()\n",
        "    for _ in range(num_tokens):\n",
        "        # Encode and pad the current input\n",
        "        encoded = [word_to_id.get(word, unk_id) for word in result[-sequence_length:]]\n",
        "        padded = tf.keras.preprocessing.sequence.pad_sequences([encoded], maxlen=sequence_length)\n",
        "\n",
        "        # Predict next word\n",
        "        pred_probs = model.predict(padded, verbose=0)[0]\n",
        "        next_id = np.random.choice(len(pred_probs), p=pred_probs)\n",
        "        next_word = id_to_word.get(next_id, \"\")\n",
        "\n",
        "        result.append(next_word)\n",
        "    return ' '.join(result)"
      ],
      "metadata": {
        "id": "1uHjn6aHMW5K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed1 = \"you will\"\n",
        "sample1 = generate_text(seed1, num_tokens=50)\n",
        "print(\"Sample 1:\")\n",
        "print(sample1)\n",
        "\n",
        "seed2 = \"while i\"\n",
        "sample2 = generate_text(seed2, num_tokens=50)\n",
        "print(\"\\nSample 2:\")\n",
        "print(sample2)"
      ],
      "metadata": {
        "id": "n5CpdqF9MoPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed280a56-3baa-4115-dfb7-402ac4f18855"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "you will contrast said   a  for and this  my to during he  whom you allowed  soon you which beloved will it  whole sought almost the however  while my of addressed fate she from i devoted the sustain from it the of   to\n",
            "\n",
            "Sample 2:\n",
            "while i father been at he  with to her  and  out circumstances  path moved latter i  have white can occupations the more  he returned spoke and was had the committed i her  my he rested the close i wonder to ever. gentleness moment for away\n"
          ]
        }
      ]
    }
  ]
}